{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Lane Finding - Project 4 of Udacity's Self-Driving Car Nanodegree\n",
    "\n",
    "1. Camera Calibration\n",
    "2. Distortion correction\n",
    "3. Image inspection\n",
    "4. Perspective correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Camera Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run AdvLaneCameraCalibrator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "camdata = open('camera_calib.pkl', 'rb')\n",
    "mtx = pickle.load(camdata)\n",
    "dist = pickle.load(camdata)\n",
    "output.close()\n",
    "\n",
    "print(mtx)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Distortion Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images = glob.glob('camera_cal/calibration*.jpg')\n",
    "\n",
    "fig = plt.figure(figsize=(16,32))\n",
    "index = 0\n",
    "\n",
    "# Step through the list and search for chessboard corners\n",
    "for fname in images:\n",
    "    img = cv2.imread(fname)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    sp = fig.add_subplot(row_count, col_count, index+1)\n",
    "    plt.imshow(img)\n",
    "    index += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example image inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "images = ['test_images/test1.jpg', 'test_images/test2.jpg', 'test_images/test3.jpg',\n",
    "          'test_images/test4.jpg', 'test_images/test5.jpg', 'test_images/test6.jpg',\n",
    "          'test_images/straight_lines1.jpg', 'test_images/straight_lines2.jpg']\n",
    "\n",
    "example_images = images\n",
    "\n",
    "col_count = 2\n",
    "row_count = 8\n",
    "fig = plt.figure(figsize=(20,60))\n",
    "\n",
    "index = 0\n",
    "\n",
    "example_image = None\n",
    "\n",
    "# Step through the list and search for chessboard corners\n",
    "for fname in example_images:\n",
    "    img = cv2.imread(fname)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    sp = fig.add_subplot(row_count, col_count, index*2+1)\n",
    "    plt.imshow(img)\n",
    "    img = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    sp = fig.add_subplot(row_count, col_count, index*2+2)\n",
    "    plt.imshow(img)\n",
    "    index += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Perspective correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example_image = cv2.imread(example_images[6])\n",
    "example_image = cv2.cvtColor(example_image, cv2.COLOR_BGR2RGB)\n",
    "example_image = cv2.undistort(example_image, mtx, dist, None, mtx)\n",
    "\n",
    "img_size = (example_image.shape[1],example_image.shape[0])\n",
    "\n",
    "# defines the perspective of the camera image by defining points near the front of the car and close the the\n",
    "# center of the image\n",
    "relation_factor = 13.5/1.92\n",
    "front_perspective_div = 3.5\n",
    "back_perspective_div = front_perspective_div*relation_factor\n",
    "front_perspective_y_perc = 0.92\n",
    "back_perspective_y_perc = 0.63\n",
    "\n",
    "# trapez points in order bottom left, top left, top right, bottom right (front, back, back, front)\n",
    "src = [(int(img_size[0]/2-img_size[0]/front_perspective_div), int(img_size[1]*front_perspective_y_perc)), \n",
    "       (int(img_size[0]/2-img_size[0]/back_perspective_div), int(img_size[1]*back_perspective_y_perc)), \n",
    "       (int(img_size[0]/2+img_size[0]/back_perspective_div), int(img_size[1]*back_perspective_y_perc)),\n",
    "       (int(img_size[0]/2+img_size[0]/front_perspective_div), int(img_size[1]*front_perspective_y_perc))]\n",
    "\n",
    "# paint trapez into the image\n",
    "image_copy = np.copy(example_image)\n",
    "\n",
    "for index in range(4):\n",
    "    pa = src[index]\n",
    "    pb = src[(index+1)%4]\n",
    "    cv2.line(image_copy, pa, pb, (255,0,0), 4)\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "plt.imshow(image_copy)\n",
    "\n",
    "margin_factor = 6\n",
    "\n",
    "dst = [(img_size[0]//margin_factor, img_size[1]), \n",
    "       (img_size[0]//margin_factor, 0), \n",
    "       (img_size[0]-img_size[0]//margin_factor, 0), \n",
    "       (img_size[0]-img_size[0]//margin_factor, img_size[1]), \n",
    "       ]\n",
    "\n",
    "src = np.array(src, dtype=np.float32)\n",
    "dst = np.array(dst, dtype=np.float32)\n",
    "\n",
    "tmx = cv2.getPerspectiveTransform(np.array(src), np.array(dst))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "width = image_copy.shape[1]\n",
    "height = image_copy.shape[0]\n",
    "\n",
    "warped = cv2.warpPerspective(example_image, tmx, (width, height))\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "plt.imshow(warped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "project_video = \"project_video.mp4\"\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(project_video))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video of the project video after distortion and perspective correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    undistorted = cv2.undistort(image, mtx, dist, None, mtx)\n",
    "    warped = cv2.warpPerspective(undistorted, tmx, (width, height))\n",
    "    return warped\n",
    "    \n",
    "from_above_video = 'test_videos_output/from_above.mp4'\n",
    "\n",
    "white_output = from_above_video\n",
    "clip1 = VideoFileClip(project_video)\n",
    "white_clip = clip1.fl_image(process_image)\n",
    "%time white_clip.write_videofile(white_output, audio=False)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(from_above_video))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highlighting lanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col_count = 2\n",
    "row_count = 8\n",
    "fig = plt.figure(figsize=(20,60))\n",
    "index = 0\n",
    "\n",
    "# Step through the list and search for chessboard corners\n",
    "for fname in example_images:\n",
    "    img = cv2.imread(fname)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    sp = fig.add_subplot(row_count, col_count, index+1)\n",
    "    img = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    plt.title(ntpath.basename(fname))\n",
    "    warped = cv2.warpPerspective(img, tmx, (width, height))\n",
    "    plt.imshow(warped, 'gray')\n",
    "    index += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def hls_threshold_mask(image, thresh=(95,255)):\n",
    "    hls = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "    \n",
    "    s = hls[:,:,2]\n",
    "    \n",
    "    binary_output = np.zeros_like(s)\n",
    "    binary_output[(s>=thresh[0]) & (s<=thresh[1])] = 1\n",
    "\n",
    "    return binary_output\n",
    "\n",
    "def sobel_mag_mask(image, thresh=(10,255), kernel_size=3):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    sobel_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=kernel_size)\n",
    "    sobel_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=kernel_size)\n",
    "    abs_sobelxy = np.sqrt(sobel_x**2 + sobel_y**2)\n",
    "    # 4) Scale to 8-bit (0 - 255) and convert to type = np.uint8\n",
    "    eight_bit = np.uint8(255*abs_sobelxy/np.max(abs_sobelxy))\n",
    "    # 5) Create a binary mask where mag thresholds are met\n",
    "    # 6) Return this mask as your binary_output image\n",
    "    binary_output = np.zeros_like(abs_sobelxy)\n",
    "    binary_output[(eight_bit >= thresh[0]) & (eight_bit <= thresh[1])] = 1    \n",
    "\n",
    "    return binary_output\n",
    "\n",
    "def sobel_dir_mask(image, thresh=(0.7, 1.3), kernel_size=13):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    sobel_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=kernel_size)\n",
    "    sobel_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=kernel_size)\n",
    "    abs_x = np.abs(sobel_x)\n",
    "    abs_y = np.abs(sobel_y)\n",
    "    # 4) Use np.arctan2(abs_sobely, abs_sobelx) to calculate the direction of the gradient \n",
    "    sob_dir = np.arctan2(abs_y, abs_x)\n",
    "    # 5) Create a binary mask where direction thresholds are met    \n",
    "    # 6) Return this mask as your binary_output image\n",
    "    binary_output = np.zeros_like(sob_dir)\n",
    "    binary_output[(sob_dir>=thresh[0]) & (sob_dir<=thresh[1])] = 1\n",
    "    \n",
    "    return binary_output\n",
    "\n",
    "def create_binary_mask(image):\n",
    "    hls_thresh = hls_threshold_mask(image)\n",
    "    sobel_mag = sobel_mag_mask(image)\n",
    "    sobel_dir = sobel_dir_mask(image)\n",
    "    \n",
    "    binary_output = np.zeros_like(sobel_dir)\n",
    "    binary_output[(hls_thresh==1)] = 1.0\n",
    "#    binary_output[(hls_thresh==1) | ((sobel_mag==1) & (sobel_dir==1))] = 1\n",
    "    ## binary_output[(hls_thresh==1)] = 1\n",
    "    \n",
    "    return binary_output\n",
    "\n",
    "col_count = 2\n",
    "row_count = 8\n",
    "fig = plt.figure(figsize=(20,60))\n",
    "index = 0\n",
    "\n",
    "# Step through the list and search for chessboard corners\n",
    "for fname in example_images:\n",
    "    img = cv2.imread(fname)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    sp = fig.add_subplot(row_count, col_count, index+1)\n",
    "    img = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    # img = sobel_mag_mask(img)\n",
    "    img = create_binary_mask(img)\n",
    "    plt.title(ntpath.basename(fname))\n",
    "    warped = cv2.warpPerspective(img, tmx, (width, height))\n",
    "    plt.imshow(warped, 'gray')\n",
    "    index += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col_count = 2\n",
    "row_count = 8\n",
    "fig = plt.figure(figsize=(20,60))\n",
    "index = 0\n",
    "\n",
    "# Step through the list and search for chessboard corners\n",
    "for fname in example_images:\n",
    "    img = cv2.imread(fname)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    sp = fig.add_subplot(row_count, col_count, index+1)\n",
    "    img = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    # img = sobel_mag_mask(img)\n",
    "    img = create_binary_mask(img)\n",
    "    plt.title(ntpath.basename(fname))\n",
    "    warped = cv2.warpPerspective(img, tmx, (width, height))\n",
    "    histogram = np.sum(warped[img.shape[0]//2:,:], axis=0)\n",
    "    plt.plot(histogram)\n",
    "    index += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for cur_fn in example_images:\n",
    "    img = cv2.imread(cur_fn)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    lf = lane_finder()\n",
    "    out_img = lf.find_lanes_using_window(img)\n",
    "    \n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    plt.imshow(out_img)\n",
    "#    plt.plot(left_fitx, ploty, color='yellow')\n",
    "#    plt.plot(right_fitx, ploty, color='yellow')\n",
    "    plt.xlim(0, 1280)\n",
    "    plt.ylim(720, 0)\n",
    "    plt.show()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lf = lane_finder()\n",
    "\n",
    "def process_image(image):\n",
    "    warped = lf.find_lanes_using_window(image)\n",
    "    return warped\n",
    "    \n",
    "find_lanes_raw = 'test_videos_output/find_lanes_raw.mp4'\n",
    "\n",
    "white_output = find_lanes_raw\n",
    "clip1 = VideoFileClip(project_video)\n",
    "white_clip = clip1.fl_image(process_image)\n",
    "%time white_clip.write_videofile(white_output, audio=False)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(find_lanes_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deepl35]",
   "language": "python",
   "name": "conda-env-deepl35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
